apiVersion: v1
data:
  Dockerfile: |-
    FROM python:3.12-slim

    WORKDIR /app
    COPY requirements.txt .

    # Install dependencies directly
    RUN pip install -r requirements.txt

    # Copy application files
    COPY serve.py .
    COPY model/ /model/

    # Set environment variables
    ENV PYTHONUNBUFFERED=1
    ENV GIT_PYTHON_REFRESH=quiet

    # Run the server
    CMD ["python", "serve.py"]
  deploy_model.py: "import os\nimport json\nimport yaml\nfrom datetime import datetime\n\ndef
    load_model_metadata():\n    \"\"\"Load model metadata from versioning step\"\"\"\n
    \   metadata_path = \"/workspace/model_metadata.json\"\n    \n    if os.path.exists(metadata_path):\n
    \       with open(metadata_path, 'r') as f:\n            return json.load(f)\n
    \   else:\n        # Fallback if metadata doesn't exist\n        return {\n            \"model_version\":
    os.getenv(\"MODEL_VERSION\", \"0.1.0\"),\n            \"timestamp\": datetime.utcnow().isoformat()
    + 'Z',\n            \"validation_results\": {\"validation_status\": \"UNKNOWN\"}\n
    \       }\n\ndef generate_seldon_deployment(image_tag, model_version, metadata):\n
    \   \"\"\"Generate SeldonDeployment manifest with versioning info\"\"\"\n    \n
    \   # Get environment variables\n    namespace = os.getenv(\"NAMESPACE\", \"argowf\")\n
    \   model_name = os.getenv(\"MODEL_NAME\", \"iris\")\n    \n    # Create deployment
    name with version\n    deployment_name = f\"{model_name}-{model_version.replace('.',
    '-')}\"\n    \n    seldon_deployment = {\n        \"apiVersion\": \"machinelearning.seldon.io/v1\",\n
    \       \"kind\": \"SeldonDeployment\",\n        \"metadata\": {\n            \"name\":
    deployment_name,\n            \"namespace\": namespace,\n            \"labels\":
    {\n                \"app\": model_name,\n                \"version\": model_version,\n
    \               \"managed-by\": \"argo-workflows\"\n            },\n            \"annotations\":
    {\n                \"deployment.timestamp\": metadata.get(\"timestamp\", \"\"),\n
    \               \"model.version\": model_version,\n                \"model.accuracy\":
    str(metadata.get(\"performance_metrics\", {}).get(\"accuracy\", \"unknown\")),\n
    \               \"validation.status\": metadata.get(\"validation_results\", {}).get(\"validation_status\",
    \"unknown\")\n            }\n        },\n        \"spec\": {\n            \"name\":
    deployment_name,\n            \"predictors\": [\n                {\n                    \"name\":
    \"default\",\n                    \"replicas\": 1,\n                    \"componentSpecs\":
    [\n                        {\n                            \"spec\": {\n                                \"containers\":
    [\n                                    {\n                                        \"name\":
    \"classifier\",\n                                        \"image\": f\"ghcr.io/jtayl222/iris:{image_tag}\",\n
    \                                       \"volumeMounts\": [\n                                            {\n
    \                                               \"name\": \"classifier-provision-location\",\n
    \                                               \"mountPath\": \"/mnt/models\"\n
    \                                           }\n                                        ],\n
    \                                       \"ports\": [\n                                            {\"containerPort\":
    8080, \"name\": \"http\", \"protocol\": \"TCP\"},  # Change from 9000 to 8080\n
    \                                           {\"containerPort\": 9500, \"name\":
    \"grpc\", \"protocol\": \"TCP\"}\n                                        ],\n
    \                                       \"env\": [\n                                            {\n
    \                                               \"name\": \"MODEL_VERSION\",\n
    \                                               \"value\": model_version\n                                            },\n
    \                                           {\"name\": \"MLSERVER_HTTP_PORT\",
    \"value\": \"8080\"}  # Change from 9000 to 8080\n                                        ],\n
    \                                       \"resources\": {\n                                            \"requests\":
    {\n                                                \"memory\": \"1Gi\",\n                                                \"cpu\":
    \"500m\"\n                                            },\n                                            \"limits\":
    {\n                                                \"memory\": \"2Gi\",\n                                                \"cpu\":
    \"1\"\n                                            }\n                                        }\n
    \                                   }\n                                ],\n                                \"initContainers\":
    [\n                                    {\n                                        \"name\":
    \"classifier-model-initializer\",\n                                        \"image\":
    \"seldonio/rclone-storage-initializer:1.17.1\",\n                                        \"args\":
    [\n                                            \"s3:models/iris/\" + model_version,
    \ # SOURCE\n                                            \"/mnt/models\"                       #
    DESTINATION\n                                        ],\n                                        \"volumeMounts\":
    [\n                                            {\n                                                \"name\":
    \"classifier-provision-location\",\n                                                \"mountPath\":
    \"/mnt/models\"\n                                            },\n                                            {\n
    \                                               \"name\": \"rclone-config\",\n
    \                                               \"mountPath\": \"/config/rclone\",\n
    \                                               \"readOnly\": True\n                                            }\n
    \                                       ],\n                                        \"env\":
    [\n                                            {\n                                                \"name\":
    \"RCLONE_CONFIG\",\n                                                \"value\":
    \"/config/rclone/rclone.conf\"\n                                            }\n
    \                                       ],\n                                        \"resources\":
    {\n                                            \"requests\": {\n                                                \"memory\":
    \"100Mi\"\n                                            },\n                                            \"limits\":
    {\n                                                \"memory\": \"1Gi\"\n                                            }\n
    \                                       }\n                                    }\n
    \                               ],\n                                \"volumes\":
    [\n                                    {\n                                        \"name\":
    \"classifier-provision-location\",\n                                        \"emptyDir\":
    {}\n                                    },\n                                    {\n
    \                                       \"name\": \"rclone-config\",\n                                        \"secret\":
    {\n                                            \"secretName\": \"rclone-config\"\n
    \                                       }\n                                    }\n
    \                               ]\n                            }\n                        }\n
    \                   ],\n                    \"graph\": {\n                        \"name\":
    \"classifier\",\n                        \"type\": \"MODEL\",\n                        \"parameters\":
    [\n                            {\n                                \"name\": \"model_uri\",\n
    \                               \"value\": \"/mnt/models\",\n                                \"type\":
    \"STRING\"\n                            }\n                        ]\n                    }\n
    \               }\n            ]\n        }\n    }\n    \n    return seldon_deployment\n\ndef
    cleanup_old_deployments(current_version, namespace=\"argowf\"):\n    \"\"\"Clean
    up old deployment versions (keep last 3)\"\"\"\n    try:\n        import subprocess\n
    \       \n        # Get all SeldonDeployments for this model\n        result =
    subprocess.run([\n            \"kubectl\", \"get\", \"seldondeployments\", \"-n\",
    namespace,\n            \"-l\", \"app=iris\", \"-o\", \"jsonpath={.items[*].metadata.name}\"\n
    \       ], capture_output=True, text=True)\n        \n        if result.returncode
    == 0:\n            deployments = result.stdout.strip().split()\n            \n
    \           # Sort deployments (assuming version-based naming)\n            deployments.sort()\n
    \           \n            # Keep only the last 3 versions, delete older ones\n
    \           if len(deployments) > 3:\n                old_deployments = deployments[:-3]\n
    \               for old_deployment in old_deployments:\n                    print(f\"\U0001F5D1️
    Cleaning up old deployment: {old_deployment}\")\n                    subprocess.run([\n
    \                       \"kubectl\", \"delete\", \"seldondeployment\", old_deployment,
    \"-n\", namespace\n                    ])\n                    \n    except Exception
    as e:\n        print(f\"⚠️ Warning: Could not clean up old deployments: {e}\")\n\ndef
    save_deployment_manifest(seldon_deployment):\n    \"\"\"Save the deployment manifest
    for kubectl apply\"\"\"\n    output_path = \"/workspace/seldon.yaml\"\n    \n
    \   with open(output_path, 'w') as f:\n        yaml.dump(seldon_deployment, f,
    default_flow_style=False)\n    \n    print(f\"✅ Seldon deployment manifest saved
    to {output_path}\")\n    return output_path\n\ndef main():\n    \"\"\"Main deployment
    logic\"\"\"\n    print(\"\U0001F680 Starting Model Deployment...\")\n    \n    try:\n
    \       # Get inputs from environment or workflow parameters\n        image_tag
    = os.getenv(\"IMAGE_TAG\", \"latest\")\n        model_version = os.getenv(\"MODEL_VERSION\",
    \"0.1.0\")\n        \n        print(f\"\U0001F4E6 Image Tag: {image_tag}\")\n
    \       print(f\"\U0001F3F7️ Model Version: {model_version}\")\n        \n        #
    Load model metadata\n        metadata = load_model_metadata()\n        print(f\"\U0001F4CA
    Model Accuracy: {metadata.get('performance_metrics', {}).get('accuracy', 'unknown')}\")\n
    \       \n        # Generate deployment manifest\n        seldon_deployment =
    generate_seldon_deployment(image_tag, model_version, metadata)\n        \n        #
    Save manifest\n        manifest_path = save_deployment_manifest(seldon_deployment)\n
    \       \n        # Apply deployment\n        import subprocess\n        print(\"\U0001F3AF
    Applying SeldonDeployment...\")\n        \n        result = subprocess.run([\n
    \           \"kubectl\", \"apply\", \"-f\", manifest_path\n        ], capture_output=True,
    text=True)\n        \n        if result.returncode == 0:\n            print(\"✅
    SeldonDeployment applied successfully!\")\n            print(result.stdout)\n
    \           \n            # Clean up old deployments\n            cleanup_old_deployments(model_version)\n
    \           \n            # Wait for deployment to be ready\n            deployment_name
    = f\"iris-{model_version.replace('.', '-')}\"\n            print(f\"⏳ Waiting
    for deployment {deployment_name} to be ready...\")\n            \n            subprocess.run([\n
    \               \"kubectl\", \"wait\", \"--for=condition=Ready\", \n                f\"seldondeployment/{deployment_name}\",\n
    \               \"-n\", os.getenv(\"NAMESPACE\", \"argowf\"),\n                \"--timeout=300s\"\n
    \           ])\n            \n            print(f\"\U0001F389 Model v{model_version}
    deployed successfully!\")\n            \n        else:\n            print(f\"❌
    Deployment failed: {result.stderr}\")\n            exit(1)\n            \n    except
    Exception as e:\n        print(f\"❌ Deployment failed: {str(e)}\")\n        exit(1)\n\nif
    __name__ == \"__main__\":\n    main()"
  monitor_model.py: "#!/usr/bin/env python3\n\"\"\"\nModel Monitoring Script for MLOps
    Pipeline\nExports metrics to Prometheus for Grafana visualization\n\"\"\"\n\nimport
    os\nimport json\nimport time\nimport logging\nimport requests  # Use requests
    instead of curl\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger
    = logging.getLogger(__name__)\n\ndef get_environment_vars():\n    \"\"\"Get environment
    variables for monitoring\"\"\"\n    return {\n        'model_version': os.getenv('MODEL_VERSION',
    'unknown'),\n        'environment': os.getenv('NAMESPACE', 'unknown'),\n        'stage':
    os.getenv('PIPELINE_STAGE', 'unknown'),\n        'pushgateway_url': os.getenv('PUSHGATEWAY_URL',
    'http://prometheus-pushgateway.monitoring.svc.cluster.local:9091')\n    }\n\ndef
    load_validation_results():\n    \"\"\"Load model validation results\"\"\"\n    validation_path
    = os.getenv('VALIDATION_RESULTS_PATH', '/workspace/validation_results.json')\n
    \   \n    if not os.path.exists(validation_path):\n        logger.warning(f\"Validation
    results not found at {validation_path}\")\n        return {}\n    \n    try:\n
    \       with open(validation_path, 'r') as f:\n            return json.load(f)\n
    \   except Exception as e:\n        logger.error(f\"Error loading validation results:
    {e}\")\n        return {}\n\ndef export_metrics_to_pushgateway():\n    \"\"\"Export
    metrics to Prometheus Pushgateway using requests\"\"\"\n    env_vars = get_environment_vars()\n
    \   validation_results = load_validation_results()\n    \n    if not validation_results:\n
    \       logger.warning(\"No validation results found, creating default metrics\")\n
    \       validation_results = {\n            'accuracy': 0.0,\n            'precision':
    0.0,\n            'recall': 0.0,\n            'f1_score': 0.0,\n            'validation_status':
    'UNKNOWN'\n        }\n    \n    # Create metrics in Prometheus format\n    metrics_data
    = f\"\"\"# MLOps Pipeline Metrics\n# TYPE iris_model_accuracy gauge\niris_model_accuracy{{version=\"{env_vars['model_version']}\",environment=\"{env_vars['environment']}\"}}
    {validation_results.get('accuracy', 0)}\n\n# TYPE iris_model_precision gauge  \niris_model_precision{{version=\"{env_vars['model_version']}\",environment=\"{env_vars['environment']}\"}}
    {validation_results.get('precision', 0)}\n\n# TYPE iris_model_recall gauge\niris_model_recall{{version=\"{env_vars['model_version']}\",environment=\"{env_vars['environment']}\"}}
    {validation_results.get('recall', 0)}\n\n# TYPE iris_model_f1_score gauge\niris_model_f1_score{{version=\"{env_vars['model_version']}\",environment=\"{env_vars['environment']}\"}}
    {validation_results.get('f1_score', 0)}\n\n# TYPE iris_pipeline_stage_success
    counter\niris_pipeline_stage_success{{version=\"{env_vars['model_version']}\",environment=\"{env_vars['environment']}\",stage=\"{env_vars['stage']}\"}}
    1\n\n# TYPE iris_model_deployment_timestamp gauge\niris_model_deployment_timestamp{{version=\"{env_vars['model_version']}\",environment=\"{env_vars['environment']}\"}}
    {time.time()}\n\"\"\"\n    \n    # Push metrics using requests with timeout\n
    \   pushgateway_url = f\"{env_vars['pushgateway_url']}/metrics/job/iris-mlops-pipeline/version/{env_vars['model_version']}/environment/{env_vars['environment']}/stage/{env_vars['stage']}\"\n
    \   \n    try:\n        logger.info(f\"Pushing metrics to {pushgateway_url}\")\n
    \       response = requests.post(\n            pushgateway_url,\n            data=metrics_data,\n
    \           headers={'Content-Type': 'text/plain; version=0.0.4; charset=utf-8'},\n
    \           timeout=10  # 10 second timeout\n        )\n        \n        if response.status_code
    == 200:\n            logger.info(\"✅ Successfully pushed metrics to Pushgateway\")\n
    \       else:\n            logger.warning(f\"⚠️ Failed to push metrics: {response.status_code}
    - {response.text}\")\n            \n    except requests.exceptions.Timeout:\n
    \       logger.error(\"❌ Timeout pushing metrics to Pushgateway\")\n    except
    requests.exceptions.ConnectionError:\n        logger.error(\"❌ Connection error
    to Pushgateway\")\n    except Exception as e:\n        logger.error(f\"❌ Error
    pushing metrics: {e}\")\n\ndef main():\n    \"\"\"Main monitoring function\"\"\"\n
    \   logger.info(\"\U0001F50D Starting MLOps monitoring...\")\n    \n    env_vars
    = get_environment_vars()\n    logger.info(f\"Environment: {env_vars}\")\n    \n
    \   # Load data\n    validation_results = load_validation_results()\n    logger.info(f\"Validation
    Results: {validation_results}\")\n    \n    # Export metrics\n    export_metrics_to_pushgateway()\n
    \   \n    logger.info(\"✅ Monitoring completed\")\n\nif __name__ == \"__main__\":\n
    \   main()"
  requirements.txt: |
    scikit-learn==1.4.2
    mlflow==2.12.1
    fastapi==0.110.0
    uvicorn[standard]==0.29.0
    boto3==1.37.34
  serve.py: |
    import pickle, os
    from fastapi import FastAPI
    import numpy as np
    import uvicorn

    model_path = os.getenv("MODEL_PATH", "/model/model.pkl")
    with open(model_path, "rb") as f:
        model = pickle.load(f)

    app = FastAPI()

    @app.post("/predict")
    def predict(payload: dict):
        data = np.array(payload["instances"])
        preds = model.predict(data).tolist()
        return {"predictions": preds}

    @app.get("/health")
    async def health():
        return {"status": "healthy"}

    @app.get("/")
    async def root():
        return {"message": "Iris classifier is running"}

    if __name__ == "__main__":
        uvicorn.run(app, host="0.0.0.0", port=8080)
  test_model.py: "import pickle\nimport numpy as np\nimport json\nimport os\nfrom
    sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score,
    classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\ndef
    load_model(model_path=None):\n    \"\"\"Load the trained model\"\"\"\n    # Check
    environment variable first, then fall back to parameter or default\n    if model_path
    is None:\n        model_path = os.getenv(\"MODEL_PATH\", \"/model/model.pkl\")\n
    \   \n    print(f\"Loading model from: {model_path}\")  # Debug info\n    \n    with
    open(model_path, \"rb\") as f:\n        return pickle.load(f)\n\ndef load_test_data():\n
    \   \"\"\"Load and split iris dataset for testing\"\"\"\n    X, y = load_iris(return_X_y=True)\n
    \   X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3,
    random_state=42, stratify=y\n    )\n    return X_test, y_test\n\ndef validate_model_accuracy(model,
    X_test, y_test, min_accuracy=0.85):\n    \"\"\"Test model accuracy meets minimum
    threshold\"\"\"\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test,
    predictions)\n    \n    print(f\"Model Accuracy: {accuracy:.4f}\")\n    print(f\"Required
    Minimum: {min_accuracy}\")\n    \n    if accuracy < min_accuracy:\n        raise
    ValueError(f\"Model accuracy {accuracy:.4f} below threshold {min_accuracy}\")\n
    \   \n    return accuracy, predictions\n\ndef validate_model_predictions(model,
    X_test):\n    \"\"\"Test model prediction format and types\"\"\"\n    predictions
    = model.predict(X_test)\n    \n    # Check prediction shape\n    assert len(predictions)
    == len(X_test), \"Prediction count mismatch\"\n    \n    # Check prediction values
    are valid class labels (0, 1, 2 for iris)\n    unique_preds = np.unique(predictions)\n
    \   assert all(pred in [0, 1, 2] for pred in unique_preds), \"Invalid prediction
    classes\"\n    \n    # Check no NaN or infinite values\n    assert not np.any(np.isnan(predictions)),
    \"NaN values in predictions\"\n    assert not np.any(np.isinf(predictions)), \"Infinite
    values in predictions\"\n    \n    print(\"✅ Model prediction validation passed\")\n
    \   return True\n\ndef validate_model_performance(model, X_test, y_test):\n    \"\"\"Comprehensive
    model performance validation\"\"\"\n    predictions = model.predict(X_test)\n
    \   \n    # Generate classification report\n    report = classification_report(y_test,
    predictions, output_dict=True)\n    conf_matrix = confusion_matrix(y_test, predictions)\n
    \   \n    # Check per-class performance\n    for class_id in [0, 1, 2]:\n        class_f1
    = report[str(class_id)]['f1-score']\n        if class_f1 < 0.7:  # Minimum F1
    score per class\n            raise ValueError(f\"Class {class_id} F1-score {class_f1:.4f}
    below threshold 0.7\")\n    \n    print(\"✅ Model performance validation passed\")\n
    \   print(\"\\nClassification Report:\")\n    print(classification_report(y_test,
    predictions))\n    \n    return report, conf_matrix\n\ndef validate_model_api_format(model,
    sample_input):\n    \"\"\"Test model works with API input format\"\"\"\n    #
    Test single prediction\n    single_pred = model.predict([sample_input])\n    assert
    len(single_pred) == 1, \"Single prediction failed\"\n    \n    # Test batch prediction\n
    \   batch_input = np.array([sample_input, sample_input])\n    batch_pred = model.predict(batch_input)\n
    \   assert len(batch_pred) == 2, \"Batch prediction failed\"\n    \n    print(\"✅
    Model API format validation passed\")\n    return True\n\ndef save_validation_results(results,
    output_path_arg=None): # Use a different name for the argument\n    \"\"\"Save
    validation results for downstream use\"\"\"\n    # Prioritize environment variable,
    then argument, then a sensible default\n    # (though the default shouldn't be
    hit if the env var is always set in the workflow)\n    actual_output_path = os.getenv(\"OUTPUT_PATH\")\n
    \   if actual_output_path is None:\n        if output_path_arg is not None:\n
    \           actual_output_path = output_path_arg\n        else:\n            #
    Fallback default, should match where Argo expects it if env var isn't set\n            actual_output_path
    = \"/workspace/validation_results.json\" \n\n    print(f\"Debug: Determined output
    path: {actual_output_path}\") # Add for debugging\n\n    # Ensure the directory
    exists\n    os.makedirs(os.path.dirname(actual_output_path), exist_ok=True)\n
    \   \n    with open(actual_output_path, \"w\") as f:\n        json.dump(results,
    f, indent=2, default=str)\n    print(f\"✅ Validation results saved to {actual_output_path}\")\n\ndef
    main():\n    \"\"\"Main validation pipeline\"\"\"\n    print(\"\U0001F9EA Starting
    Model Validation Tests...\")\n    \n    try:\n        # Load model and test data\n
    \       model = load_model()\n        X_test, y_test = load_test_data()\n        \n
    \       # Run validation tests\n        accuracy, predictions = validate_model_accuracy(model,
    X_test, y_test)\n        validate_model_predictions(model, X_test)\n        report,
    conf_matrix = validate_model_performance(model, X_test, y_test)\n        validate_model_api_format(model,
    X_test[0])\n        \n        # Compile results\n        results = {\n            \"validation_status\":
    \"PASSED\",\n            \"accuracy\": float(accuracy),\n            \"classification_report\":
    report,\n            \"confusion_matrix\": conf_matrix.tolist(),\n            \"test_count\":
    len(X_test),\n            \"timestamp\": str(np.datetime64('now'))\n        }\n
    \       \n        # Save results\n        save_validation_results(results)\n        \n
    \       print(f\"\\n\U0001F389 All validation tests PASSED!\")\n        print(f\"Model
    accuracy: {accuracy:.4f}\")\n        \n    except Exception as e:\n        print(f\"\\n❌
    Validation FAILED: {str(e)}\")\n        \n        # Save failure results\n        failure_results
    = {\n            \"validation_status\": \"FAILED\",\n            \"error\": str(e),\n
    \           \"timestamp\": str(np.datetime64('now'))\n        }\n        save_validation_results(failure_results)\n
    \       \n        # Exit with error code\n        exit(1)\n\nif __name__ == \"__main__\":\n
    \   main()"
  train.py: "import mlflow, os, pickle, json\nfrom sklearn.datasets import load_iris\nfrom
    sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import
    RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\nmlflow.set_experiment(\"iris_demo\")\n\nwith
    mlflow.start_run():\n    X, y = load_iris(return_X_y=True)\n    X_tr, X_te, y_tr,
    y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    n_estimators
    = int(os.getenv(\"N_ESTIMATORS\", 100))\n    clf = RandomForestClassifier(n_estimators=n_estimators,
    random_state=42)\n    clf.fit(X_tr, y_tr)\n\n    acc = accuracy_score(y_te, clf.predict(X_te))\n
    \   mlflow.log_param(\"n_estimators\", n_estimators)\n    mlflow.log_metric(\"accuracy\",
    acc)\n\n    # Save model to /output/model/ (not /tmp/model/)\n    os.makedirs(\"/output/model\",
    exist_ok=True)\n    model_path = \"/output/model/model.pkl\"\n    print(f\"Saving
    model to {model_path}\")\n    with open(model_path, \"wb\") as f:\n        pickle.dump(clf,
    f)\n    mlflow.log_artifacts(\"/output/model\")\n\n    # Write run info\n    run_id
    = mlflow.active_run().info.run_id\n    with open(\"/output/run_info.json\", \"w\")
    as f:\n        json.dump({\"run_id\": run_id}, f)\n    \n    print(\"Training
    complete with accuracy:\", acc)\n"
  version_model.py: "import json\nimport os\nimport requests\nimport semver\nfrom
    datetime import datetime\n\ndef get_current_version():\n    \"\"\"Get the current
    version from Git tags or start with v0.1.0\"\"\"\n    try:\n        # In a real
    scenario, you'd query Git tags or a version registry\n        # For demo, we'll
    simulate with a simple logic\n        \n        # Try to get last version from
    a hypothetical registry/git\n        # For now, we'll use a simple file-based
    approach\n        version_file = \"/workspace/last_version.txt\"\n        \n        if
    os.path.exists(version_file):\n            with open(version_file, 'r') as f:\n
    \               last_version = f.read().strip()\n            return last_version\n
    \       else:\n            # Start with initial version\n            return \"0.1.0\"\n
    \           \n    except Exception as e:\n        print(f\"Warning: Could not
    determine current version: {e}\")\n        return \"0.1.0\"\n\ndef load_validation_results():\n
    \   \"\"\"Load validation results from the validation step\"\"\"\n    results_path
    = os.getenv(\"VALIDATION_RESULTS_PATH\", \"/workspace/validation_results.json\")\n
    \   \n    with open(results_path, 'r') as f:\n        return json.load(f)\n\ndef
    determine_version_bump(validation_results, current_version):\n    \"\"\"Determine
    what type of version bump is needed\"\"\"\n    \n    accuracy = validation_results.get('accuracy',
    0.0)\n    validation_status = validation_results.get('validation_status', 'FAILED')\n
    \   \n    # Parse current version\n    try:\n        version_info = semver.VersionInfo.parse(current_version)\n
    \   except:\n        version_info = semver.VersionInfo.parse(\"0.1.0\")\n    \n
    \   print(f\"Current version: {version_info}\")\n    print(f\"Validation status:
    {validation_status}\")\n    print(f\"Model accuracy: {accuracy:.4f}\")\n    \n
    \   # Version bump logic based on validation results\n    if validation_status
    == \"FAILED\":\n        print(\"❌ Validation failed - no version bump\")\n        return
    None\n    \n    # Determine bump type based on accuracy improvement\n    if accuracy
    >= 0.99:\n        # Exceptional accuracy - minor bump\n        print(\"\U0001F680
    Exceptional accuracy (≥99%) - MINOR version bump\")\n        new_version = version_info.bump_minor()\n
    \   elif accuracy >= 0.95:\n        # Good accuracy - patch bump\n        print(\"✅
    Good accuracy (≥95%) - PATCH version bump\")\n        new_version = version_info.bump_patch()\n
    \   elif accuracy >= 0.85:\n        # Acceptable accuracy - patch bump\n        print(\"✅
    Acceptable accuracy (≥85%) - PATCH version bump\")\n        new_version = version_info.bump_patch()\n
    \   else:\n        # Low accuracy - no version bump\n        print(\"⚠️ Low accuracy
    (<85%) - no version bump\")\n        return None\n    \n    return str(new_version)\n\ndef
    create_model_metadata(validation_results, version):\n    \"\"\"Create comprehensive
    model metadata\"\"\"\n    \n    timestamp = datetime.utcnow().isoformat() + 'Z'\n
    \   \n    metadata = {\n        \"model_version\": version,\n        \"timestamp\":
    timestamp,\n        \"validation_results\": validation_results,\n        \"performance_metrics\":
    {\n            \"accuracy\": validation_results.get('accuracy', 0.0),\n            \"test_samples\":
    validation_results.get('test_count', 0),\n            \"validation_status\": validation_results.get('validation_status',
    'UNKNOWN')\n        },\n        \"model_info\": {\n            \"algorithm\":
    \"RandomForestClassifier\",\n            \"dataset\": \"iris\",\n            \"target_classes\":
    [\"setosa\", \"versicolor\", \"virginica\"],\n            \"features\": [\"sepal_length\",
    \"sepal_width\", \"petal_length\", \"petal_width\"]\n        },\n        \"deployment_info\":
    {\n            \"container_registry\": \"ghcr.io/jtayl222/iris\",\n            \"serving_framework\":
    \"FastAPI + Seldon\",\n            \"kubernetes_ready\": True\n        }\n    }\n
    \   \n    return metadata\n\ndef save_version_info(version, metadata):\n    \"\"\"Save
    version information for downstream steps\"\"\"\n    \n    # Save version for workflow
    output\n    version_path = os.getenv(\"OUTPUT_PATH\", \"/workspace/model_version.txt\")\n
    \   with open(version_path, 'w') as f:\n        f.write(version)\n    \n    #
    Save version tag for container tagging\n    version_tag_path = os.getenv(\"VERSION_TAG_PATH\",
    \"/workspace/version_tag.txt\")\n    with open(version_tag_path, 'w') as f:\n
    \       f.write(f\"v{version}\")  # Add 'v' prefix for container tags\n    \n
    \   # Save complete metadata\n    metadata_path = \"/workspace/model_metadata.json\"\n
    \   with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n
    \   \n    # Save current version for next run\n    with open(\"/workspace/last_version.txt\",
    'w') as f:\n        f.write(version)\n    \n    print(f\"✅ Version information
    saved:\")\n    print(f\"   Version: {version}\")\n    print(f\"   Container Tag:
    v{version}\")\n    print(f\"   Metadata: {metadata_path}\")\n\ndef main():\n    \"\"\"Main
    versioning logic\"\"\"\n    print(\"\U0001F3F7️ Starting Model Versioning...\")\n
    \   \n    try:\n        # Load validation results\n        validation_results
    = load_validation_results()\n        print(f\"✅ Loaded validation results\")\n
    \       \n        # Get current version\n        current_version = get_current_version()\n
    \       print(f\"\U0001F4CB Current version: {current_version}\")\n        \n
    \       # Determine version bump\n        new_version = determine_version_bump(validation_results,
    current_version)\n        \n        if new_version is None:\n            print(\"❌
    No version bump - using current version\")\n            new_version = current_version\n
    \       else:\n            print(f\"\U0001F3AF New version: {new_version}\")\n
    \       \n        # Create metadata\n        metadata = create_model_metadata(validation_results,
    new_version)\n        \n        # Save version info\n        save_version_info(new_version,
    metadata)\n        \n        print(f\"\\n\U0001F389 Model versioning completed!\")\n
    \       print(f\"Version: {new_version}\")\n        print(f\"Container Tag: v{new_version}\")\n
    \       \n    except Exception as e:\n        print(f\"❌ Versioning failed: {str(e)}\")\n
    \       \n        # Save fallback version\n        fallback_version = \"0.1.0\"\n
    \       with open(os.getenv(\"OUTPUT_PATH\", \"/workspace/model_version.txt\"),
    'w') as f:\n            f.write(fallback_version)\n        with open(os.getenv(\"VERSION_TAG_PATH\",
    \"/workspace/version_tag.txt\"), 'w') as f:\n            f.write(f\"v{fallback_version}\")\n
    \       \n        exit(1)\n\nif __name__ == \"__main__\":\n    main()"
kind: ConfigMap
metadata:
  name: iris-src
  namespace: argowf
