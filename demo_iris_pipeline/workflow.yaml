apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: iris-demo
spec:
  entrypoint: iris-pipeline
  # Add volumes at workflow level
  volumes:
  - name: src
    configMap:
      name: iris-src
  volumeClaimTemplates:
  - metadata:
      name: workdir
    spec:
      storageClassName: nfs-client
      accessModes: [ReadWriteMany]
      resources:
        requests:
          storage: 1Gi
  templates:
  - name: iris-pipeline
    dag:
      tasks:
      - name: train
        template: train
      - name: build-image
        dependencies: [train]
        template: kaniko
      - name: deploy
        dependencies: [build-image]
        template: deploy
        arguments:
          parameters:
          - name: image-tag
            value: '{{tasks.build-image.outputs.parameters.image-tag}}'

  - name: train
    container:
      image: jupyter/scipy-notebook:python-3.11
      # Add resource requests and limits
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"
      command: ["/bin/bash", "-c"]
      args:
        - |
          # Fix the backports issue
          pip uninstall -y backports.tarfile
          pip install backports.tarfile

          # Install setuptools first, before anything else
          pip install setuptools==78.1.0 boto3==1.37.34
          
          # Copy requirements.txt from configmap
          cp /src/requirements.txt /tmp/requirements.txt
          
          # Install packages from requirements.txt
          pip install -r /tmp/requirements.txt
          
          # Run the training script
          python /src/train.py
      volumeMounts:
      - name: workdir
        mountPath: /output
      - name: src
        mountPath: /src
      env:
      - name: MLFLOW_TRACKING_URI
        value: "http://mlflow.mlflow.svc.cluster.local:5000"
      - name: GIT_PYTHON_REFRESH
        value: "quiet"
      envFrom:
      - secretRef:
          name: minio-credentials
    volumes:
    - name: src
      configMap:
        name: iris-src

  - name: kaniko
    outputs:
      parameters:
      - name: image-tag
        valueFrom:
          path: /workspace/image_tag.txt
    container:
      image: gcr.io/kaniko-project/executor:v1.23.0
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"
      args:
      - --context=/workspace
      - --dockerfile=/workspace/Dockerfile
      # Use a hardcoded tag that matches what we'll set in the init container
      - --destination=ghcr.io/jtayl222/iris:latest
      - --tarPath=/tmp/image.tar
      - --push-retry=5
      - --verbosity=debug
      - --no-push
      volumeMounts:
      - name: workdir
        mountPath: /workspace
      - name: src  # Mount configmap that contains Dockerfile
        mountPath: /src
      - name: workdir  # Mount the same volume from train step to access the model
        mountPath: /output
      env:
      - name: GITHUB_TOKEN
        valueFrom:
          secretKeyRef:
            name: github-credentials
            key: token
            optional: true
    initContainers:
    - name: prepare-workspace
      image: busybox
      resources:
        requests:
          memory: "512Mi"
          cpu: "500m"
        limits:
          memory: "1Gi"
          cpu: "1"
      command: ["/bin/sh", "-c"]
      args:
      - |
        # Copy Dockerfile and model to workspace
        cp /src/Dockerfile /workspace/
        cp /src/requirements.txt /workspace/
        
        # Debug output to check if model exists in output directory
        echo "Listing output directory:"
        ls -la /output
        echo "Listing output/model directory:"
        ls -la /output/model || echo "No model directory in output"
        
        # Make sure model directory exists in workspace
        mkdir -p /workspace/model
        
        # Copy model from output volume (adjust path based on how train.py saves it)
        # Try different possible locations where the model might be
        if [ -f "/output/model/model.pkl" ]; then
          cp /output/model/model.pkl /workspace/model/
        elif [ -f "/output/model.pkl" ]; then
          mkdir -p /workspace/model
          cp /output/model.pkl /workspace/model/
        elif [ -f "/output/workdir/model/model.pkl" ]; then
          cp /output/workdir/model/model.pkl /workspace/model/
        else
          echo "Model file not found in expected locations"
          find /output -name "model.pkl" -type f
          exit 1
        fi
        
        cp /src/serve.py /workspace/
        
        # Generate a fixed tag for now (we'll use "latest")
        TAG="latest"
        echo "Generated tag: $TAG"
        
        # Save tag for output parameter
        echo "$TAG" > /workspace/image_tag.txt
        
        echo "Final workspace contents:"
        ls -la /workspace
        ls -la /workspace/model || echo "No model directory in workspace"
      volumeMounts:
      - name: workdir
        mountPath: /workspace
      - name: src
        mountPath: /src
      - name: workdir
        mountPath: /output

  - name: deploy
    inputs:
      parameters:
      - name: image-tag
    container:
      image: bitnami/kubectl:1.30
      resources:
        requests:
          memory: "256Mi"
          cpu: "200m"
        limits:
          memory: "512Mi"
          cpu: "500m"
      command: [sh, -c]
      args:
      - |
        cat <<EOF > /tmp/seldon.yaml
        apiVersion: machinelearning.seldon.io/v1
        kind: SeldonDeployment
        metadata:
          name: iris
          namespace: argowf
        spec:
          predictors:
          - name: default
            componentSpecs:
            - spec:
                containers:
                - name: classifier
                  image: ghcr.io/jtayl222/iris:{{inputs.parameters.image-tag}}
                  imagePullPolicy: Always
            graph:
              name: classifier  # Add this required field
              type: MODEL
              endpoint:
                type: REST
              parameters:
              - name: model_uri
                type: STRING
                value: /model/model.pkl
        EOF
        kubectl apply -f /tmp/seldon.yaml
