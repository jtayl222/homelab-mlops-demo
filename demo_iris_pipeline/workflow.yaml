apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: iris-demo
spec:
  entrypoint: iris-pipeline
  # Add volumes at workflow level
  volumes:
  - name: src
    configMap:
      name: iris-src
  volumeClaimTemplates:
  - metadata:
      name: workdir
    spec:
      storageClassName: nfs-shared
      accessModes: [ReadWriteMany]
      resources:
        requests:
          storage: 1Gi
  templates:
  - name: iris-pipeline
    dag:
      tasks:
      - name: train
        template: train
      - name: validate
        template: model-validation
        dependencies: [train]
      - name: kaniko
        template: kaniko
        dependencies: [validate] 
      - name: deploy
        template: deploy
        dependencies: [kaniko]
        arguments:
          parameters:
          - name: image-tag
            value: "{{tasks.kaniko.outputs.parameters.image-tag}}"

  - name: train
    container:
      image: jupyter/scipy-notebook:python-3.11
      securityContext:
        runAsUser: 0
        runAsGroup: 0
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"
      command: ["/bin/bash", "-c"]
      args:
        - |
          # Fix permissions on the output directory
          set -x
          mount
          df -h /output
          chown -R 1000:100 /output
          
          # Switch to jovyan user with proper environment
          su jovyan -c "
          source /opt/conda/etc/profile.d/conda.sh
          conda activate base
          pip uninstall -y backports.tarfile
          pip install backports.tarfile
          pip install setuptools==78.1.0 boto3==1.37.34
          cp /src/requirements.txt /tmp/requirements.txt
          pip install -r /tmp/requirements.txt
          python /src/train.py
          "
      volumeMounts:
      - name: workdir
        mountPath: /output
      - name: src
        mountPath: /src
      env:
      - name: MLFLOW_TRACKING_URI
        value: "http://mlflow.mlflow.svc.cluster.local:5000"
      - name: GIT_PYTHON_REFRESH
        value: "quiet"
      envFrom:
      - secretRef:
          name: minio-credentials-wf
    volumes:
    - name: src
      configMap:
        name: iris-src

  - name: kaniko
    outputs:
      parameters:
      - name: image-tag
        valueFrom:
          path: /workspace/image_tag.txt
    container:
      image: gcr.io/kaniko-project/executor:v1.23.0
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"
      args:
      - --context=/workspace
      - --dockerfile=/workspace/Dockerfile
      - --destination=ghcr.io/jtayl222/iris:latest
      - --push-retry=5
      - --verbosity=debug
      volumeMounts:
      - name: workdir
        mountPath: /workspace
      - name: src
        mountPath: /src
      - name: workdir
        mountPath: /output
      - name: docker-config
        mountPath: /kaniko/.docker
        readOnly: true
      env:
      - name: DOCKER_CONFIG
        value: /kaniko/.docker
    volumes:
    - name: docker-config
      secret:
        secretName: ghcr-secret
        items:
        - key: .dockerconfigjson
          path: config.json
    initContainers:
    - name: prepare-workspace
      image: busybox
      resources:
        requests:
          memory: "512Mi"
          cpu: "500m"
        limits:
          memory: "1Gi"
          cpu: "1"
      command: ["/bin/sh", "-c"]
      args:
      - |
        # Copy Dockerfile and model to workspace
        cp /src/Dockerfile /workspace/
        cp /src/requirements.txt /workspace/
        
        # Debug output to check if model exists in output directory
        echo "Listing output directory:"
        ls -la /output
        echo "Listing output/model directory:"
        ls -la /tmp/model || echo "No model directory in output"
        
        # Make sure model directory exists in workspace
        mkdir -p /workspace/model
        
        # Copy model from output volume
        if [ -f "/output/model/model.pkl" ]; then
          cp /output/model/model.pkl /workspace/model/
        else
          echo "Model file not found in expected locations"
          find /output -name "model.pkl" -type f
          exit 1
        fi
        
        cp /src/serve.py /workspace/
        
        # Generate a fixed tag for now (we'll use "latest")
        TAG="latest"
        echo "Generated tag: $TAG"
        
        # Save tag for output parameter
        echo "$TAG" > /workspace/image_tag.txt
        
        echo "Final workspace contents:"
        ls -la /workspace
        ls -la /workspace/model || echo "No model directory in workspace"
      volumeMounts:
      - name: workdir
        mountPath: /workspace
      - name: src
        mountPath: /src
      - name: workdir
        mountPath: /output

  - name: model-validation
    outputs:
      parameters:
      - name: validation-status
        valueFrom:
          path: /workspace/validation_results.json  # Changed from /output
    container:
      image: python:3.12-slim
      resources:
        requests:
          memory: "1Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "1000m"
      volumeMounts:
      - name: workdir
        mountPath: /workspace  # Only mount workdir once
      - name: src
        mountPath: /src
      command: [sh, -c]
      args:
      - |
        cd /workspace
        
        # Install dependencies
        pip install scikit-learn==1.5.1 numpy pandas
        
        # Model is already in /workspace/model.pkl from training step
        # Copy and run validation script
        cp /src/test_model.py .
        
        # Set environment variables for validation script
        export MODEL_PATH=/workspace/model.pkl
        export OUTPUT_PATH=/workspace/validation_results.json
        
        python test_model.py
        
        echo "Model validation completed"

  - name: deploy
    inputs:
      parameters:
      - name: image-tag
    container:
      image: bitnami/kubectl:1.30
      resources:
        requests:
          memory: "256Mi"
          cpu: "200m"
        limits:
          memory: "512Mi"
          cpu: "500m"
      command: [sh, -c]
      args:
      - |
        cat <<EOF > /tmp/seldon.yaml
        apiVersion: machinelearning.seldon.io/v1
        kind: SeldonDeployment
        metadata:
          name: iris
          namespace: argowf
        spec:
          predictors:
          - name: default
            graph:
              name: classifier
              type: MODEL
              children: []
            componentSpecs:
            - spec:
                imagePullSecrets:
                - name: ghcr-secret
                containers:
                - name: classifier
                  image: ghcr.io/jtayl222/iris:{{inputs.parameters.image-tag}}
                  imagePullPolicy: Always
                  env:                              # ADD THIS SECTION
                  - name: PREDICTIVE_UNIT_SERVICE_PORT     # Tell Seldon what port to use
                    value: "8080"
                  - name: PREDICTIVE_UNIT_HTTP_SERVICE_PORT
                    value: "8080"  
                  ports:
                  - containerPort: 8080
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8080
                    initialDelaySeconds: 10
                    periodSeconds: 5
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8080
                    initialDelaySeconds: 30
                    periodSeconds: 10
        EOF
        kubectl apply -f /tmp/seldon.yaml


