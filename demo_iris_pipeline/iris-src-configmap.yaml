apiVersion: v1
data:
  Dockerfile: |-
    FROM python:3.12-slim

    WORKDIR /app
    COPY requirements.txt .

    # Install dependencies directly
    RUN pip install -r requirements.txt

    # Copy application files
    COPY serve.py .
    COPY model/ /model/

    # Set environment variables
    ENV PYTHONUNBUFFERED=1
    ENV GIT_PYTHON_REFRESH=quiet

    # Run the server
    CMD ["python", "serve.py"]
  requirements.txt: |
    scikit-learn==1.4.2
    mlflow==2.12.1
    fastapi==0.110.0
    uvicorn[standard]==0.29.0
    boto3==1.37.34
  serve.py: |
    import pickle, os
    from fastapi import FastAPI
    import numpy as np
    import uvicorn

    model_path = os.getenv("MODEL_PATH", "/model/model.pkl")
    with open(model_path, "rb") as f:
        model = pickle.load(f)

    app = FastAPI()

    @app.post("/predict")
    def predict(payload: dict):
        data = np.array(payload["instances"])
        preds = model.predict(data).tolist()
        return {"predictions": preds}

    if __name__ == "__main__":
        uvicorn.run(app, host="0.0.0.0", port=8080)
  train.py: |
    import mlflow, os, pickle, json
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score

    mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])  # http://mlflow.mlflow.svc:5000
    mlflow.set_experiment("iris_demo")

    with mlflow.start_run():
        X, y = load_iris(return_X_y=True)
        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)

        n_estimators = int(os.getenv("N_ESTIMATORS", 100))
        clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
        clf.fit(X_tr, y_tr)

        acc = accuracy_score(y_te, clf.predict(X_te))
        mlflow.log_param("n_estimators", n_estimators)
        mlflow.log_metric("accuracy", acc)

        # save artefact for downstream image build
        os.makedirs("/tmp/model", exist_ok=True)
        model_path = "/tmp/model/model.pkl"
        print(f"Saving model to {model_path}")
        with open(model_path, "wb") as f:
            pickle.dump(clf, f)
        mlflow.log_artifacts("/tmp/model")

        # write a tiny JSON so the next step knows where the artefact is
        run_id = mlflow.active_run().info.run_id
        with open("/output/run_info.json", "w") as f:
            json.dump({"run_id": run_id}, f)
  workflow.yaml: "apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name:
    iris-demo\nspec:\n  entrypoint: iris-pipeline\n  # Add volumes at workflow level\n
    \ volumes:\n  - name: src\n    configMap:\n      name: iris-src\n  volumeClaimTemplates:\n
    \ - metadata:\n      name: workdir\n    spec:\n      storageClassName: nfs-shared\n
    \     accessModes: [ReadWriteMany]\n      resources:\n        requests:\n          storage:
    1Gi\n  templates:\n  - name: iris-pipeline\n    dag:\n      tasks:\n      - name:
    train\n        template: train\n      - name: build-image\n        dependencies:
    [train]\n        template: kaniko\n      - name: deploy\n        dependencies:
    [build-image]\n        template: deploy\n        arguments:\n          parameters:\n
    \         - name: image-tag\n            value: '{{tasks.build-image.outputs.parameters.image-tag}}'\n\n
    \ - name: train\n    container:\n      image: jupyter/scipy-notebook:python-3.11\n
    \     # Add resource requests and limits\n      resources:\n        requests:\n
    \         memory: \"2Gi\"\n          cpu: \"1\"\n        limits:\n          memory:
    \"4Gi\"\n          cpu: \"2\"\n      command: [\"/bin/bash\", \"-c\"]\n      args:\n
    \       - |\n          # Fix the backports issue\n          pip uninstall -y backports.tarfile\n
    \         pip install backports.tarfile\n\n          # Install setuptools first,
    before anything else\n          pip install setuptools==78.1.0 boto3==1.37.34\n
    \         \n          # Copy requirements.txt from configmap\n          cp /src/requirements.txt
    /tmp/requirements.txt\n          \n          # Install packages from requirements.txt\n
    \         pip install -r /tmp/requirements.txt\n          \n          # Run the
    training script\n          python /src/train.py\n      volumeMounts:\n      -
    name: workdir\n        mountPath: /output\n      - name: src\n        mountPath:
    /src\n      env:\n      - name: MLFLOW_TRACKING_URI\n        value: \"http://mlflow.mlflow.svc.cluster.local:5000\"\n
    \     - name: GIT_PYTHON_REFRESH\n        value: \"quiet\"\n      envFrom:\n      -
    secretRef:\n          name: minio-credentials-wf\n    volumes:\n    - name: src\n
    \     configMap:\n        name: iris-src\n\n  - name: kaniko\n    outputs:\n      parameters:\n
    \     - name: image-tag\n        valueFrom:\n          path: /workspace/image_tag.txt\n
    \   container:\n      image: gcr.io/kaniko-project/executor:v1.23.0\n      resources:\n
    \       requests:\n          memory: \"2Gi\"\n          cpu: \"1\"\n        limits:\n
    \         memory: \"4Gi\"\n          cpu: \"2\"\n      args:\n      - --context=/workspace\n
    \     - --dockerfile=/workspace/Dockerfile\n      # Use a hardcoded tag that matches
    what we'll set in the init container\n      - --destination=ghcr.io/jtayl222/iris:latest\n
    \     - --tarPath=/tmp/image.tar\n      - --push-retry=5\n      - --verbosity=debug\n
    \     - --no-push\n      volumeMounts:\n      - name: workdir\n        mountPath:
    /workspace\n      - name: src  # Mount configmap that contains Dockerfile\n        mountPath:
    /src\n      - name: workdir  # Mount the same volume from train step to access
    the model\n        mountPath: /output\n      env:\n      - name: GITHUB_TOKEN\n
    \       valueFrom:\n          secretKeyRef:\n            name: github-credentials\n
    \           key: token\n            optional: true\n    initContainers:\n    -
    name: prepare-workspace\n      image: busybox\n      resources:\n        requests:\n
    \         memory: \"512Mi\"\n          cpu: \"500m\"\n        limits:\n          memory:
    \"1Gi\"\n          cpu: \"1\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n
    \     - |\n        # Copy Dockerfile and model to workspace\n        cp /src/Dockerfile
    /workspace/\n        cp /src/requirements.txt /workspace/\n        \n        #
    Debug output to check if model exists in output directory\n        echo \"Listing
    output directory:\"\n        ls -la /output\n        echo \"Listing output/model
    directory:\"\n        ls -la /tmp/model || echo \"No model directory in output\"\n
    \       \n        # Make sure model directory exists in workspace\n        mkdir
    -p /workspace/model\n        \n        # Copy model from output volume (adjust
    path based on how train.py saves it)\n        # Try different possible locations
    where the model might be\n        if [ -f \"/tmp/model/model.pkl\" ]; then\n
    \         cp /tmp/model/model.pkl /workspace/model/\n        elif [ -f \"/tmp/model.pkl\"
    ]; then\n          mkdir -p /workspace/model\n          cp /tmp/model.pkl /workspace/model/\n
    \       elif [ -f \"/output/workdir/model/model.pkl\" ]; then\n          cp /output/workdir/model/model.pkl
    /workspace/model/\n        else\n          echo \"Model file not found in expected
    locations\"\n          find /output -name \"model.pkl\" -type f\n          exit
    1\n        fi\n        \n        cp /src/serve.py /workspace/\n        \n        #
    Generate a fixed tag for now (we'll use \"latest\")\n        TAG=\"latest\"\n
    \       echo \"Generated tag: $TAG\"\n        \n        # Save tag for output
    parameter\n        echo \"$TAG\" > /workspace/image_tag.txt\n        \n        echo
    \"Final workspace contents:\"\n        ls -la /workspace\n        ls -la /workspace/model
    || echo \"No model directory in workspace\"\n      volumeMounts:\n      - name:
    workdir\n        mountPath: /workspace\n      - name: src\n        mountPath:
    /src\n      - name: workdir\n        mountPath: /output\n\n  - name: deploy\n
    \   inputs:\n      parameters:\n      - name: image-tag\n    container:\n      image:
    bitnami/kubectl:1.30\n      resources:\n        requests:\n          memory: \"256Mi\"\n
    \         cpu: \"200m\"\n        limits:\n          memory: \"512Mi\"\n          cpu:
    \"500m\"\n      command: [sh, -c]\n      args:\n      - |\n        cat <<EOF >
    /tmp/seldon.yaml\n        apiVersion: machinelearning.seldon.io/v1\n        kind:
    SeldonDeployment\n        metadata:\n          name: iris\n          namespace:
    argowf\n        spec:\n          predictors:\n          - name: default\n            componentSpecs:\n
    \           - spec:\n                containers:\n                - name: classifier\n
    \                 image: ghcr.io/jtayl222/iris:{{inputs.parameters.image-tag}}\n
    \                 imagePullPolicy: Always\n            graph:\n              name:
    classifier  # Add this required field\n              type: MODEL\n              endpoint:\n
    \               type: REST\n              parameters:\n              - name: model_uri\n
    \               type: STRING\n                value: /model/model.pkl\n        EOF\n
    \       kubectl apply -f /tmp/seldon.yaml\n"
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: iris-src
